---
title: "STA6257 Data Science Capstone Project"
author: "Jesse Buntyn"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

### Week 2 Individual Work

#### Source 1 - Cross Validation, by Daniel Berrar ([@berrar2019cross])

Cross-validation, as described in Daniel Berrar's paper, is a vital
resampling method in data science for assessing the predictive
performance of models. It helps in avoiding overfitting and
underfitting, ensuring that models generalize well to unseen data.
Common types include:

Single Hold-Out Method: Divides data into a training set and a test set,
usually in a 70-30 or 90-10 split.

K-Fold Cross-Validation: The dataset is split into k equally sized
folds. Each fold is used once as a test set while the rest serve as the
training set.

Leave-One-Out Cross-Validation (LOOCV): A special case of k-fold
cross-validation where k equals the number of data points. Each data
point is used once as a test set.

Stratified Cross-Validation: Ensures each fold reflects the class
proportions in the dataset, particularly useful for imbalanced datasets.

Cross-validation is crucial for parameter tuning and model selection. It
provides a more robust measure of model performance compared to a single
split, as it averages performance across multiple splits. This ensures
the model's ability to perform consistently across different subsets of
the dataset.

The selection of a particular cross-validation method depends on the
dataset's size and nature. For large datasets, k-fold cross-validation
is generally preferred, while LOOCV is suitable for smaller datasets
despite its higher computational cost. The key is to balance the
bias-variance tradeoff and ensure that the model's predictive
performance is not overly optimistic. [@berrar2019cross]

#### Source 2 - Selecting a classification method by cross-validation, by Cullen Schaffer ([@schaffer1993selecting])

In "Selecting a Classification Method by Cross-Validation," Cullen Schaffer provides an in-depth exploration of cross-validation, a critical technique in evaluating machine learning models. Cross-validation helps determine how well a model will generalize to an independent data set. It involves partitioning the data into complementary subsets, performing the analysis on one subset (training set), and validating the analysis on the other subset (test set).

Schaffer emphasizes the role of cross-validation in the context of selecting machine learning classification methods. The paper delves into various cross-validation methods, notably:

K-Fold Cross-Validation: This method involves dividing the data into k subsets. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. The process is repeated k times, with each of the k subsets used exactly once as the test set.

Leave-One-Out Cross-Validation (LOOCV): This is a special case of cross-validation where the number of folds equals the number of instances in the dataset. It is particularly useful for small datasets but can be computationally intensive.

Stratified Cross-Validation: Especially useful in dealing with imbalanced datasets. This method ensures that each fold of the dataset contains approximately the same percentage of samples of each target class as the complete set.

Schaffer's analysis is crucial for understanding the nuances of these techniques and their application in real-world scenarios, highlighting the importance of choosing the right method based on the dataset and the specific requirements of the model. [@schaffer1993selecting]

### Week 3 Individual Work

#### Source 1 - Making Sense of Model Generalizability: A Tutorial on Cross-Validation in R and Shiny, by Song, Wang, & Wee ([@song2021making])

The document "Making Sense of Model Generalizability: A Tutorial on Cross-Validation in R and Shiny" by Song, Tang, and Wee provides a comprehensive tutorial on cross-validation as a statistical technique to assess model generalizability. The paper delineates a structured, five-step approach to cross-validation, emphasizing the division of data into training and testing sets, model fitting and parameter estimation, prediction accuracy assessment, repetition of these steps, and the computation of average prediction accuracy across repetitions.

Furthermore, the article expands on the important concept of k-folds cross-validation, particularly the repeated k-folds technique. This approach involves multiple iterations of k-folds cross-validation, each with a unique partitioning of the data, enhancing the unbiased nature of model predictors. However, this method's thoroughness comes at the cost of increased time and labor, potentially escalating overall costs.


Cross-Validation Process:

Data Partitioning: Distinguish between training and testing datasets.

Model Training: Fit the model on the training dataset and derive the model parameters.

Model Testing: Apply the trained model to the test dataset to determine predictive accuracy.
Reiteration: Perform steps 1 to 3 multiple times.

Accuracy Estimation: Compute the average prediction accuracy from all iterations for cross-validation. 


Repeated k-Folds Technique:

Data Division: Randomly split the dataset into k equal parts.

Training Phase: Use (k-1) portions of the data for model training.

Prediction and Validation: Predict the response variable in the unused kth portion and assess prediction error.

Cycling Through Folds: Repeat steps 2 and 3 for each of the k segments.

Multiple Iterations: Execute the entire process (steps 1-4) n times.

Performance Evaluation: Average the modelâ€™s performance indicators across all repetitions for a comprehensive model assessment. [@song2021making]

#### Source 2 - A K-fold Averaging Cross-validation Procedure, by Yoonsuh Jung and Jianhua Hu ([@jung2015ak])

"A K-fold Averaging Cross-validation Procedure" by Jung and Hu, offers an advanced perspective on cross-validation methods with a focus on k-fold averaging cross-validation (ACV) in linear models. The authors propose a procedure for model selection and parameter estimation, where instead of selecting a single 'optimal' model from the k-folds, an ultimate model is derived by averaging the parameter estimates from the 'optimal' models of each fold. This method is postulated to yield more stable and efficient parameter estimates than the classical k-fold cross-validation (CV) approach.

The article discusses the application of this procedure to high-dimensional data through penalization methods like LASSO and quantile smoothing splines, illustrating the flexibility and broad applicability of the ACV method. Theoretical properties are established, showing the asymptotic equivalence between the ACV and classical CV procedures in linear regression settings.

In terms of the mathematical methodology, the article provides important equations for calculating the mean squared prediction error (MSPE) and mean squared error (MSE) as metrics to evaluate model performance. For instance, the overall test predictors are evaluated using the MSPE, calculated as:

$$
MSPE = \frac{1}{K} \sum_{i=1}^{K} MSE_i
$$

where MSEi is the mean squared error for the ith fold, and K is the number of folds.

For the k-fold cross-validation process, the steps outlined in the document are as follows:

1. Randomly and evenly split the dataset into k-folds.

2. Use k-1 folds as the training set to fit the model.

3. Predict the response variable in the hold-out (kth) fold using the fitted model.

4. Calculate the prediction error for the response variable in the hold-out fold.

5. Repeat steps 2-4 k times, using each fold as a hold-out once.

6. Calculate the overall test predictors by averaging all k test predictors.

The ACV method further develops the k-folds technique by averaging the parameter estimates from the selected 'optimal' models of each fold to create the ultimate model. This approach potentially reduces the variance of the parameter estimates, leading to more robust model selection and parameter estimation. However, the efficacy of ACV compared to CV is conditional on the true underlying model and the complexity of the model, as well as the sample size and number of folds chosen.

In summary, the article presents an innovative take on k-fold cross-validation, aiming to improve model selection and parameter estimation by introducing an averaging step to the procedure, and showcasing its benefits through theoretical discussion and empirical examples. [@jung2015ak]

### Week 4 Individual Work

#### Source 1 - Cross-validation for selecting a model selection procedure, by Zhang and Yang (2015) ([@zhang2015cross])


Zhang and Yang's 2015 work delves into the utilization of cross-validation for the purpose of model selection. They explore the division of data to optimize model fitting and performance evaluation, emphasizing the challenges in high-dimensional contexts. The discourse includes the comparison of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), with an emphasis on the choice between parametric or non-parametric models. The paper addresses the strategic implementation of cross-validation, recognizing that while fixed ratios are theoretically ideal, the actual impact on cross-validation performance can vary. 

They demonstrate, through simulations, how a larger evaluation set can influence the reliability of cross-validation outcomes. Furthermore, they present a concept known as the 'half-half' ratio in non-parametric scenarios, suggesting it can lead to more accurate model selection. The paper also confronts common misunderstandings regarding cross-validation and suggests a synthesis of AIC and BIC methods to achieve asymptotically optimal estimations in varied settings. [@zhang2015cross]

#### Source 2 - Reliable Accuracy Estimates from k-Fold Cross Validation, by Yong and Weh (2020) ([@wong2019reliable])

In their 2020 study, Wong and Yeh examine the repeated use of k-fold cross-validation for deriving reliable accuracy estimates in classification algorithms. They highlight that the variance of accuracy estimates tends to decrease with multiple replications, which some argue could lead to more reliable estimates. However, the study also notes the potential pitfalls of this approach, such as the underestimation of variance due to the correlation between replications, which previous studies may have overlooked.

The researchers propose new statistical methods to evaluate the dependency of accuracy estimates and test the strength of these dependencies. Their experiments across 20 datasets indicate that accuracy estimates from different replications of k-fold cross-validation are generally highly correlated, especially as the number of folds increases. They recommend using a larger number of folds with fewer replications for performance evaluation. Particularly, their research provides evidence that ten-fold cross-validation yields the most significant scores among k-fold cross-validation techniques. Ultimately, their findings challenge the notion that simply increasing the number of replications will inherently lead to more reliable accuracy estimates, bringing attention to the importance of considering the dependency relationships between replications.[@wong2019reliable]

## Introduction

This is an introduction to understanding cross-validation, its methods, and its importance in data science and statistics. 

Cross-validation is a cornerstone methodology in the field of data science, essential for assessing the predictive performance of statistical models and ensuring their generalizability to unseen data. This resampling technique allows researchers to evaluate how models will perform in practice, addressing critical challenges like overfitting and underfitting, thereby ensuring robustness and reliability of model predictions across various data subsets.

Daniel Berrar's (2019) exposition on cross-validation underscores its importance in parameter tuning and model selection, providing a detailed overview of its common types, including k-fold and leave-one-out cross-validation, and emphasizing its role in balancing the bias-variance tradeoff for improved model performance [@berrar2019cross]. This perspective is enriched by Cullen Schaffer (1993), who explores the significance of cross-validation in selecting optimal classification methods, highlighting its necessity in dealing with imbalanced datasets through stratified approaches [@schaffer1993selecting].

The tutorial by Song, Tang, and Wee (2021) further elaborates on the practical application of cross-validation, presenting a structured, step-by-step approach to assessing model generalizability and emphasizing the significance of the repeated k-folds technique for reducing bias in model evaluation [@song2021making].

Yoonsuh Jung and Jianhua Hu (2015) introduce an advanced k-fold averaging cross-validation procedure, advocating for the averaging of parameter estimates across folds to yield more stable and efficient outcomes. This method demonstrates significant potential in handling high-dimensional data and improving parameter estimation [@jung2015ak].

Zhang and Yang (2015) delve into the strategic use of cross-validation for model selection, particularly in high-dimensional settings, challenging traditional approaches and proposing the integration of AIC and BIC for optimal model estimation [@zhang2015cross]. AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are both measures used to evaluate the quality of a statistical model, often in the context of model selection. While they are not directly used for cross-validation, they serve a similar purpose in helping to choose between different models by considering both the goodness of fit and the complexity of the model. Likewise, Wong and Yeh (2019) contribute to the discourse by examining the reliability of accuracy estimates derived from k-fold cross-validation, cautioning against the uncritical increase of replications without considering the correlation between them, and recommending specific strategies for achieving more dependable accuracy estimates [@wong2019reliable].

Collectively, these papers illuminate the multifaceted nature of cross-validation, showcasing its critical role in model evaluation, selection, and generalization. From theoretical underpinnings to practical implementations, cross-validation emerges as an essential tool in a data scientist's arsenal, ensuring models are not only predictive but also generalizable across diverse datasets and contexts.


## Methods

In terms of the mathematical methodology, the article by Jung and Hu (2015) provides important equations for calculating the mean squared prediction error (MSPE) and mean squared error (MSE) as metrics to evaluate model performance. For instance, the overall test predictors are evaluated using the MSPE, calculated as:

$$
MSPE = \frac{1}{K} \sum_{i=1}^{K} MSE_i
$$

where MSEi is the mean squared error for the ith fold, and K is the number of folds [@jung2015ak].

## Analysis and Results

### Data and Vizualisation

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

### Conlusion

### References
