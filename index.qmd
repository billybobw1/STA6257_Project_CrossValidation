---
title: "Cross-Validation - Methods & Importance in Data Science and Statistics"
author: "Jesse Buntyn, Jolie Wise, Danielle Koche"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to understanding cross-validation, its methods,
and its importance in data science and statistics.

Cross-validation is a cornerstone methodology in the field of data
science, essential for assessing the predictive performance of
statistical models and ensuring their generalizability to unseen data.
This resampling technique allows researchers to evaluate how models will
perform in practice, addressing critical challenges like overfitting and
underfitting, thereby ensuring robustness and reliability of model
predictions across various data subsets.

The tutorial by [@song2021making] further elaborates on the practical
application of cross-validation, presenting a structured, step-by-step
approach to assessing model generalizability and emphasizing the
significance of the repeated k-folds technique for reducing bias in
model evaluation.

**The overall 5-step process to cross-validation introduced by them is
as follows:**

1.  Split the data into a training set and test set, typically 80/20 or
    70/30.
2.  Fit a model to the training set and obtain the model parameters.
3.  Apply the fitted model to the test set and obtain prediction
    accuracy.
4.  Repeat steps one through three.
5.  Calculate the average cross-validation prediction accuracy across
    all repetitions.

[@berrar2019cross] exposition on cross-validation underscores its
importance in parameter tuning and model selection, providing a detailed
overview of its common types, including k-fold and leave-one-out
cross-validation, and emphasizing its role in balancing the
bias-variance tradeoff for improved model performance.

**Specifically, common types of cross-validation include:**

**Single Hold-Out Method:** This is one of the more simple methods of
cross-validation and tends to be less time-consuming to conduct
[@yadav2016analysis]. It divides data into a training set and a test
set, usually in a 70-30 or 90-10 split.

**Monte Carlo cross-validation:** Using random sampling without
replacement from certain probability distributions when splitting your
data sets [@james1980monte]. It aims to decrease overfitting by allowing
the algorithm to better explore the experimental space.

**K-Fold Cross-Validation:** The dataset is split into k equally sized
folds. Each fold is used once as a test set while the rest serve as the
training set.

**Leave-One-Out Cross-Validation (LOOCV):** This is a special case of
k-fold cross-validation where k equals the number of data points
[@shao2016efficient]. Each data point is used once as a test set.

**Stratified Cross-Validation:** Ensures each fold reflects the class
proportions in the dataset, particularly useful for imbalanced datasets.

**Repeated K-Fold Cross-Validation:** Repeats the K-Fold process
multiple times and averages the results.

The perspective of Berrar is enriched by [@schaffer1993selecting], who
explores the significance of cross-validation in selecting optimal
classification methods, highlighting its necessity in dealing with
imbalanced datasets through stratified approaches.

### Related Work

[@zhang2015cross] delve into the strategic use of cross-validation for
model selection, particularly in high-dimensional settings, challenging
traditional approaches and proposing the integration of AIC and BIC for
optimal model estimation. AIC (Akaike Information Criterion) and BIC
(Bayesian Information Criterion) are both measures used to evaluate the
quality of a statistical model, often in the context of model selection.
While they are not directly used for cross-validation, they serve a
similar purpose in helping to choose between different models by
considering both the goodness of fit and the complexity of the model.

Likewise, [@wong2019reliable] contribute to the discourse by examining
the reliability of accuracy estimates derived from k-fold
cross-validation, cautioning against the uncritical increase of
replications without considering the correlation between them, and
recommending specific strategies for achieving more dependable accuracy
estimates. More specifically, their experiments across 20 datasets
indicate that accuracy estimates from different replications of k-fold
cross-validation are generally highly correlated, especially as the
number of folds increases. They instead recommend using a larger number
of folds with fewer replications for performance evaluation.
Particularly, their research concludes that ten-fold cross-validation
yields the most significant scores among k-fold cross-validation
techniques. A similar conclusion of the optimal for k is shared by
[@marcot2021optimal], as their evidence from their study that explores
the ideal value of k for k-fold cross-validation analysis showed that
the ideal value of k is 10, however, 5 can be sufficient in some cases.

Ultimately, in statistical analysis, researchers are constantly
innovating and refining cross-validation techniques to enhance model
performance and address challenges associated with existing
methodologies. [@filzmoser2009repeated] sought to explore a method
intended to improve the reliability and predictive accuracy of complex
linear models, particularly those developed from small datasets, and to
provide more accurate prediction error estimates for future
observations. Their investigation was centered around the concept of
repeated double cross-validation (rdCV), an advanced technique that
entails a dual-phase partitioning of the dataset.

The initial phase involves dividing the data into training and testing
sets, which serves as the foundation for prediction accuracy estimation.
Subsequently, the second phase employs the training set from the first
division in a further round of cross-validation. This dual-loop process
is repeated multiple iterations, generating a comprehensive set of
predicted outcomes. The aggregate of these outcomes not only facilitates
the enhancement of the model's performance but also aids in the more
precise estimation of the variability in the model's predictions.

Another technique, introduced by [@xu2018representative] is
Representative Splitting Cross-Validation (RSCV), aiming to enhance the
representativeness and uniformity of calibration and validation sets in
experimental studies, with an emphasis on its utility for latent
variable selection. This method employs the DUPLEX algorithm for data
segmentation, followed by k-fold cross-validation on the divided sets.

Building on this, [@rabinowicz2022cross] explore the influence of
correlation structures on cross-validation effectiveness, proposing a
bias-correction measure, CVc, to address potential biases. This
correction is unnecessary when dataset splitting maintains the
distributional relationship between training and testing sets, ensuring
unbiased cross-validation. Their analysis showed that CVc more
accurately estimates generalization error compared to traditional
cross-validation, remaining unbiased provided the covariance structure
is correctly specified, even without the ideal prediction method. This
research contributes to refining cross-validation techniques,
particularly in handling complex data correlations.

Finally, [@lei2020cross] introduces a method called Cross-Validation
with Confidence (CVC) aimed at reducing overfitting, a common problem
with traditional cross-validation that uses small splits and ends up
with too-simple models. CVC works by checking the predicted risk of
different models and using p-values to choose models that best fit
without being overly complex. It has proven effective, particularly when
dealing with a decent amount of data and models that don't use too many
predictors, resulting in simpler, easy-to-understand predictive models.
However, CVC hasn't yet been applied to unsupervised learning or models
with binary outcomes, pointing out areas for further exploration.

Collectively, these papers illuminate the multifaceted nature of
cross-validation, showcasing its critical role in model evaluation,
selection, and generalization. From theoretical underpinnings to
practical implementations, cross-validation emerges as an essential tool
in a data scientist's arsenal, ensuring models are not only predictive
but also generalizable across diverse datasets and contexts.

## Methods

In terms of the mathematical methodology, the article by Jung and Hu
(2015) provides important equations for calculating the mean squared
prediction error (MSPE) and mean squared error (MSE) as metrics to
evaluate model performance.

The MSE is the expectation of the squared norm of the difference between
the estimated and the true parameter vectors, and can be calculated as:

$$
MSE = \sum_{i=1}^{p} (\hat{\beta}_i - \beta_i)^2
$$

where $\hat{\beta}_i$ is the estimated value of the $i$-th parameter,
$\beta_i$ is the true value of the $i$-th parameter, and $p$ is the
total number of parameters. The lower the MSE, the better the model's
predictions match the actual data.

The overall test predictors are evaluated using the MSPE, calculated as:

$$
MSPE = \frac{1}{K} \sum_{i=1}^{K} MSE_i
$$

where MSEi is the mean squared error for the ith fold, and K is the
number of folds [@jung2015ak].

Related, in statistical modeling, $R^2$, also known as the coefficient
of determination, is a measure that indicates the proportion of the
variance in the dependent variable that is predictable from the
independent variables. It provides a sense of how well the independent
variables predict the dependent variable; the closer $R^2$ is to 1, the
better the model explains the data.

This can be written as:

$$
R^2 = 1 - \frac{\sum{(y_i - \hat{y})^2}}{\sum{(y_i - \bar{y})^2}}
$$

where $\bar{y}$ is the mean value of $y$ [@ott2015introduction].

Ultimately, $R^2$ and MSE are related in that they both measure the
model's performance, but they do it in different ways. While $R^2$
measures the proportion of variance explained by the model, MSE measures
the average error of the model's predictions. In the context of
cross-validation, both metrics can be used to compare models: a higher
$R^2$ and a lower MSE would typically indicate a better predictive
model.

## Cross-validation Techniques

There are many techniques of cross-validation, the one being used
depending on what needs to be accomplished. Many of which, mentioned in
the introduction. The following will introduce some of those important
techniques, as well as others, and research that delve deeper into their
purpose.

[@jung2015ak] introduce an advanced k-fold averaging cross-validation
procedure, advocating for the averaging of parameter estimates across
folds to yield more stable and efficient outcomes. This method
demonstrates significant potential in handling high-dimensional data and
improving parameter estimation.

**For the k-fold cross-validation process, the steps are as follows and
see Figure 1:**

1.  Randomly and evenly split the dataset into k-folds.

2.  Use k-1 folds as the training set to fit the model.

3.  Predict the response variable in the hold-out (kth) fold using the
    fitted model.

4.  Calculate the prediction error for the response variable in the
    hold-out fold.

5.  Repeat steps 2-4 k times, using each fold as a hold-out once.

6.  Calculate the overall test predictors by averaging all k test
    predictors.

    ![Figure 1: Basic K-Fold Cross-Validation,
    k=5](images/K-fold-cross-validation-1.webp){width="413"}

    Source: [@Ranjan_2021]

The MSE calculation using K-folds is as follows [@jung2015ak]:

$$
CV(k) = \frac{1}{k} \sum_{i=1}^{k} MSE_i
$$

**For clarity, the difference between K-folds and repeated K-folds,
another method mentioned earlier, is as follows:**

Repeated K-Fold Cross-Validation enhances the robustness of K-Fold by
repeating the process multiple times and averaging the results. This
method helps to reduce the variability of the single trial of K-Fold
cross-validation, providing a more accurate estimate of the model's
ability to generalize to unseen data. Here's the process:

1.  **Repeated Splitting:** The data is split into k folds as before,
    but this process is repeated n times, with a different random split
    each time.

2.  **Model Evaluation:** Just like in K-Fold, models are trained and
    evaluated k times on each repetition with different splits, leading
    to n\*k total evaluations.

3.  **Final Estimation:** The scores across all repeats are aggregated
    to provide a final measure.

Notably, this technique of repeated K-Folds is especially useful when
the data set is not too large and there is a need to ensure the
stability of the evaluation metrics across different splits of the data.

Monte Carlo Cross-Validation (MCCV) is another commonly used method.
Similar to the k-folds technique, MCCV randomly removes significant
chunks of the dataset without replacement to use as its training and
validation ($s_v$) data sets [@haddad2013applicability]. This sampling
is repeated N times. The criterion for this techniques, according to
[@haddad2013applicability], is as follows:

$$
MCCV_{n_v}(\phi)=\frac{1}{Nn_v}\sum_{i=1}^{k}(y_{(s_v)(i)}-\hat{y}_{\phi(s_v)(i)})^2
$$

One of the appeals of MCCV over some other techniques its reduced
computational complexity compared to other techniques, such as
Leave-One-Out Cross-Validation[@james1980monte].

## Analysis and Results

### Data Description and Visualization

This heart failure clinical records dataset, obtained via the UC Irvine
Machine Learning Repository, contains the medical records of 299
patients who had heart failure, collected during their follow-up period,
where each patient profile has 13 clinical features.

More specifically, the dataset comprises medical records of 299 heart
failure patients from Faisalabad, Pakistan, collected between April and
December 2015. It includes 13 clinical, body, and lifestyle features.
The dataset was used to facilitate machine learning predictions on
patient survival based solely on serum creatinine and ejection fraction
levels. Various machine learning and biostatistical methods were
employed for binary classification of survival, feature ranking, and to
examine the predictability of survival using the mentioned key features.
This study aims to demonstrate the potential of specific biomarkers in
predicting heart failure outcomes and to streamline the prediction
process for medical practitioners. Notably, [@de2020cross] highlight the
ways in which cross-validation can be implemented on a dataset, such as
ours, through different cross-validation methods and techniques.

Our data description is as follows:

| **Variable**           | **Type**  | **Characteristic**                    |
|------------------------|-----------|---------------------------------------|
| Age                    | Predictor | Integer                               |
| Anaemia                | Predictor | Binary (1 = anemic, 0 = not)          |
| creatine_phosphokinase | Predictor | Integer                               |
| Diabetes               | Predictor | Binary (1 = diabetic, 0 = not)        |
| ejection_fraction      | Predictor | Integer                               |
| high_blood_pressure    | Predictor | Binary (1 = HBP present, 0 = no HBP)  |
| Platelets              | Predictor | Continuous                            |
| serum_creatinine       | Predictor | Continuous                            |
| serum_sodium           | Predictor | Integer                               |
| Sex                    | Predictor | Binary (1 = M, 0 = F)                 |
| Smoking                | Predictor | Binary (1 = smoker, 2 = not a smoker) |
| Time                   | Predictor | Integer                               |
| death_event            | Response  | Binary (1 = death, 0 = no death)      |

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(ggplot2)
library(corrplot)
library(rsample)
library(tidymodels)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
data <- read.csv('heart_failure_clinical_records_dataset.csv')
```

#### Continuous Variables

```{r}
par(mfrow=c(2,4))
hist(data$age, main="Age")
hist(data$creatinine_phosphokinase, main="Creatinine Phosphokinase")
hist(data$ejection_fraction, main="Ejection Fraction")
hist(data$platelets, main="Platelets")
hist(data$serum_creatinine, main="Serum Creatinine")
hist(data$serum_sodium, main="Serum Sodium")
hist(data$time, main="Time")
```

#### Categorical Data

```{r}
par(mfrow=c(2,3))
hist(data$anaemia, main="Anaemia", breaks=2)
hist(data$diabetes, main="Diabetes", breaks=2)
hist(data$high_blood_pressure, main="High Blood Pressure", breaks=2)
hist(data$sex, main="Sex", breaks=2)
hist(data$smoking, main = "Smoking", breaks = 2)
hist(data$DEATH_EVENT,main = "Death Event",breaks=2)
```

#### Missing Values

```{r}
colSums(is.na(data)) #total missing values per column
```

#### Checking for Outliers

```{r}
#Boxplots to look for outliers
par(mfrow=c(2,4))
boxplot(data$age, main="Age")
boxplot(data$creatinine_phosphokinase, main="Creatinine Phosphokinase")
boxplot(data$ejection_fraction, main="Ejection Fraction")
boxplot(data$platelets, main="Platelets")
boxplot(data$serum_creatinine, main="Serum Creatinine")
boxplot(data$serum_sodium, main="Serum Sodium")
boxplot(data$time, main="Time")
```

#### Correlation

```{r}
cor(data) %>%
  corrplot(method = 'ellipse', type = 'upper')

par(mfrow=c(2,4))
plot(x=data$age,y=data$DEATH_EVENT, main="Age", ylim = c(-.5,1.5))
plot(x=data$creatinine_phosphokinase,y=data$DEATH_EVENT, main="Creatinine Phosphokinase",ylim = c(-.5,1.5))
plot(x=data$ejection_fraction,y=data$DEATH_EVENT, main="Ejection Fraction",ylim = c(-.5,1.5))
plot(x=data$platelets,y=data$DEATH_EVENT, main="Platelets",ylim = c(-.5,1.5))
plot(x=data$serum_creatinine,y=data$DEATH_EVENT, main="Serum Creatinine",ylim = c(-.5,1.5))
plot(x=data$serum_sodium,y=data$DEATH_EVENT, main="Serum Sodium",ylim = c(-.5,1.5))
plot(x=data$time,y=data$DEATH_EVENT, main="Time",ylim = c(-.5,1.5))
```

### Statistical Modeling

#### 10-fold Cross Validation:

```{r}
library(boot)

set.seed(43269)
m1 <- glm(DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure + platelets + serum_creatinine + serum_sodium + sex + smoking + time,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m1, K=10)$delta

set.seed(85329)
m2 <- glm(DEATH_EVENT ~ ejection_fraction + serum_creatinine,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m2, K=10)$delta
```

With any variation of k-fold cross validation, the dataset is split into
k sized folds that will all be equal. Here, the dataset is split in 10
folds. As previously discussed, [@wong2019reliable] mentions 10-fold
cross validation analysis to be most effective in most cases. In model
1, all predictor variables are included. In model 2, only ejection
fraction and serum creatinine are included, as the researchers that
provided the data set came to the conclusion that death events could be
predicted using those 2 variables alone. Interestingly enough, the first
model that included all variables produced a lower MSE (mean square
error), indicating that from this CV test, model one would be better to
use than model 2.

#### 5-fold Cross Validation:

```{r}
library(boot)

set.seed(143269)
m1 <- glm(DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure + platelets + serum_creatinine + serum_sodium + sex + smoking + time,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m1, K=5)$delta


set.seed(185329)
m2 <- glm(DEATH_EVENT ~ ejection_fraction + serum_creatinine,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m2, K=5)$delta
```

Similar to the 10-fold models above, our dataset is split into equal
pieces. However, in this case, we are only doing 5 folds compared to our
10-fold analysis above. Again, in model 1, all predictor variables are
included. In model 2, only ejection fraction and serum creatinine are
included. With 5-fold cross validation, the data set is divided into 5
parts (as opposed to 10 in 10-fold). Model 1 still has a lower MSE than
the full model that includes all predictor variables.

#### Single Hold-Out Method:

```{r}
#add obs # for each row
library(dplyr)
data_SHO <- data %>%
  mutate(obs = row_number()) %>%
  relocate(obs)

#split
library(rsample)
set.seed(726159)
split_SHO <- initial_split(data_SHO, prop = 0.5)
training_SHO <- training(split_SHO)
validation_SHO <- testing(split_SHO)

#create SHO models with training set
m1_SHO <- glm(DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure + platelets + serum_creatinine + serum_sodium + sex + smoking + time,
         data = training_SHO, 
         family = "binomial"(link = "logit"))

m2_SHO <- glm(DEATH_EVENT ~ ejection_fraction + serum_creatinine,
         data = training_SHO, 
         family = "binomial"(link = "logit"))

#calculate MSE
validation_SHO <- validation_SHO %>% 
  mutate(yhat_m1_SHO = predict(m1_SHO, newdata = validation_SHO),
         yhat_m2_SHO = predict(m2_SHO, newdata = validation_SHO)) %>%
  mutate(sqerr_m1_SHO = (yhat_m1_SHO - DEATH_EVENT)^2,
         sqerr_m2_SHO = (yhat_m2_SHO - DEATH_EVENT)^2) %>%
  relocate(obs, DEATH_EVENT, yhat_m1_SHO, sqerr_m1_SHO, yhat_m2_SHO, sqerr_m2_SHO)

head(validation_SHO)

mean(validation_SHO$sqerr_m1_SHO, na.rm = TRUE)
mean(validation_SHO$sqerr_m2_SHO, na.rm = TRUE)
```

Using the Single Hold-Out Method, the data set is split into a training
set and a test/validation set. Here, we are using a 50-50 split. After
conducting the analysis, it can be seen that model 2 provides us with a
lower mean squared error. Model 2 includes only ejection fraction and
serum creatinine as predictor variables, meaning this reduced model is a
better fit according to this cross-validation technique.

#### Leave-One-Out Cross-Validation (LOOCV):

```{r}
set.seed(7432)
m1 <- glm(DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + diabetes + ejection_fraction + high_blood_pressure + platelets + serum_creatinine + serum_sodium + sex + smoking + time,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m1)$delta


set.seed(20987)
m2 <- glm(DEATH_EVENT ~ ejection_fraction + serum_creatinine,
         data = data, 
         family = "binomial"(link = "logit"))

cv.glm(data, m2)$delta
```

Leave-One-Out cross-validation (LOOCV) follows a similar data set
splitting technique, but in this case, the data is split k-ways, where k
is equal to the number of data points. According to LOOCV, the full
model (which includes all predictor variables) is a better fit due to a
lower mean squared error compared to the reduced model (which only
includes serum creatinine and ejection fraction).

#### Monte Carlo Cross-Validation:

```{r}
data$DEATH_EVENT=as.factor(data$DEATH_EVENT)
set.seed(30)
resample1 <- mc_cv(data, times = 100, prop = .5)
map_dbl(
  resample1$splits,
  function(x) {
    dat <- as.data.frame(x)$DEATH_EVENT
    mean(dat == "Yes")
  }
)

m1_recipe <- recipe(DEATH_EVENT ~ ., data = data)
mccv_model <- logistic_reg() %>% #creating classification model
  set_engine("glm") %>%
  set_mode("classification")
m1_wflow <- #adding variables for model 1
  workflow() %>%
  add_formula(
    DEATH_EVENT ~ .) %>%
  add_model(mccv_model)
m2_wflow <- # adding variables for model 2
  workflow() %>%
  add_formula(
    DEATH_EVENT ~ ejection_fraction + serum_creatinine) %>%
  add_model(mccv_model)



keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
set.seed(30)
m1_MCCV <-m1_wflow %>% fit_resamples(resamples = resample1, control = keep_pred)
collect_metrics(m1_MCCV)
m2_MCCV <- m2_wflow %>% fit_resamples(resamples = resample1, control = keep_pred)
collect_metrics(m2_MCCV)
```

## Conclusion

This paper shows various cross-validation techniques that can be used for modeling validation, including single hold-out, 5 & 10 k-folds, leave-one-out, and Monte Carlo cross-validation.

While the researchers that provided the dataset found that survival of
heart attack patients can be predicted using only serum creatinine and
ejection fraction as predictor variables. The linear regression analysis we conducted using
various cross-validation techniques has shown that the full model (which
includes all predictor variables within the dataset) has a lower mean
squared error (MSE) after testing. This essentially means the full model
is more robust with predictive ability compared to the reduced model.
That being said, the difference in MSEs between the two models is marginal, however, the
full model by technicality does have slightly more predictive ability.

The marginal difference between models supports the argument made by the
researchers because the model with two predictor variables and the model
with twelve predictor variables produced small differences in MSEs
as seen in the analysis above; models with less predictor variables are typically preferred in statistical analysis. 

Ultimately, while the k-folds CV technique (particularly 10 folds) seemed to perform the best, most of these techniques seem sufficient to help validate a model based on this example.

### References
